{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker\n",
    "\n",
    "__SUMMARY__\n",
    "> docker system prune\n",
    "\n",
    "> docker system df  ----> disk usage summary\n",
    "\n",
    "> docker system info  ----> complete info\n",
    "\n",
    "> docker rmi\n",
    "\n",
    "> docker rm\n",
    "\n",
    "> docker volume rm\n",
    "\n",
    "### Purging all dangling images, container, volumes and networks\n",
    "\n",
    "`docker system prune` _Prune means to remove unwanted stuff_\n",
    "\n",
    "`docker system prune -a` _To remove all_\n",
    "\n",
    "> Attention: prune will not remove the running containers. Use `docker info` to get detailed info about containers, volumes, images and other specs\n",
    "\n",
    "## ----------------------------------- CONTAINERS -----------------------------------\n",
    "\n",
    "### List all containers\n",
    "`docker ps -a`\n",
    "\n",
    "`docker ps -aq` [q: only ids]\n",
    "\n",
    "### Stop all running containers\n",
    "`docker stop $(docker ps -aq)`\n",
    "\n",
    "### Remove all containers\n",
    "`docker rm $(docker ps -aq)`\n",
    "\n",
    "### Run and remove\n",
    "`docker run -rm image_name`\n",
    "\n",
    "### Remove only exited containers\n",
    "`docker rm $(docker ps -aq -f status=exited)`\n",
    "\n",
    "### Remove containers according to pattern\n",
    "`docker ps -a | grep \"pattern\" | awk '{print $1}' | xargs docker rm`\n",
    "> awk and xargs to supply the ID to docker rm\n",
    "\n",
    "## ----------------------------------- IMAGES -----------------------------------\n",
    "\n",
    "### List images\n",
    "`docker images` or `docker image ls`\n",
    "\n",
    "### Remove all images\n",
    "`docker rmi $(docker images -aq)` _q: for id_\n",
    "\n",
    "### List dangling images\n",
    "`docker images -f dangling=true`\n",
    "\n",
    "### Remove dangline images\n",
    "`docker image prune`\n",
    "\n",
    "### Remove images according to pattern\n",
    "`docker images -a | grep \"pattern\" | awk '{print $3}' | xargs docker rmi`\n",
    "\n",
    "\n",
    "## ----------------------------------- VOLUMES -----------------------------------\n",
    "\n",
    "### List volumes\n",
    "`docker volume ls`\n",
    "\n",
    "### Remove volume\n",
    "`docker volume rm vol_name`\n",
    "\n",
    "### List dangling volumes\n",
    "`docker volume ls -f dangling=true`\n",
    "\n",
    "### Remove dangling volumes\n",
    "\n",
    "Removes unused volumes i.e volumes not used by any container\n",
    "\n",
    "`docker volume prune <--force>`\n",
    "\n",
    "> With --force, it will not ask for confirmation\n",
    "\n",
    "> Even if container stops, docker will conside the volume to be in use\n",
    "\n",
    "## Remove container and its volume\n",
    "`docker rm -v container_name`\n",
    "\n",
    "Summary of docker volume subcommands\n",
    "```\n",
    "docker volume create\n",
    "docker volume ls\n",
    "docker volume inspect\n",
    "docker volume rm\n",
    "docker volume prune\n",
    "```\n",
    "\n",
    "As per the recommendations, I should keep two things in mind on _DATA PERSISTENCE_:\n",
    "\n",
    "1. Use volume instead of bind mount or tmpfs (temporary file storage: stores data in RAM)\n",
    "2. Use `--mount source=<name of volume created>,target=<absolute destination in container>`\n",
    "3. We can use bind mount when our data is in some other directory (instead of /var/lib/docker) and we want to mount that on docker container\n",
    "\n",
    "-----------\n",
    "\n",
    "\n",
    "### To run bash on docker image\n",
    "```bash\n",
    "# Assuming an Ubuntu Docker image\n",
    "$ docker run -it --name <container_name> <image> /bin/bash\n",
    "```\n",
    "\n",
    "> This won't work if your image has a defined ENTRYPOINT. For these cases use: `docker run -it --entrypoint /bin/bash <image>`\n",
    "\n",
    "### To go inside container running in detached state\n",
    "`docker exec -it cc73eb6d6f75 bash`\n",
    "\n",
    "------------\n",
    "\n",
    "## Theory on Docker and Docker Layer Caching\n",
    "\n",
    "This is where the paradigm shift comes into place: software is no longer packaged as a platform-specific binary artifact (jar, dll, tgz) but as a fully fledged virtual environment in the form of Docker images. This means that developers can run the code locally exactly as it is run on dev, test or prod environments. The operations teams have only Docker images to deal with, with much less need to understand the inner workings of the specific platform they are deploying.\n",
    "\n",
    "Every time you change or update the application code, you need to build a new version of the image that can be used for deployment. Even though you’d typically only change the application code, the entire image needs to be built from scratch — including all dependencies. To help with the efficiency of the typical development process and shorten the feedback loop cycle, Docker has introduced the concept of layer caching.\n",
    "\n",
    "## Theory on Docker Compose and Buildkit\n",
    "\n",
    "We can use `docker build` and `docker run` to build and test/run our applications. But if we have multiple docker files for e.g for client app and one for server then to switch back and forth and build and run these can be cumbersome. To solve this, we have `docker-compose` that can build and run with just one command.\n",
    "\n",
    "With the help of Buildkit (still experimental) we can use caching layer service of docker build into docker-compose. When we use docker build and then docker compose the docker compose builds the dockerfile from scratch and doesn't use the cache of docker build that we ran first time. With the help of buildkit command docker-compose will re-use the cache layer.\n",
    "\n",
    "Note: when we use builtkit for the first time, it'll create it's own layer storage strategy. This means it'll build the docker image from scratch but then after that when we use docker-compose, it will use the buildkit cache storage and would not build from scratch.\n",
    "\n",
    "Brief introduction [5mins read]: https://medium.com/better-programming/sharing-cached-layer-between-docker-and-docker-compose-builds-c2e5f751cee4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker Contd.\n",
    "\n",
    "## Share data between docker container and host\n",
    "\n",
    "## Bind mount volumes\n",
    "\n",
    "Docker containers are emphemeral meaning any data created inside the container is only available in that container and only while the container is running.\n",
    "\n",
    "__Scenario__:\n",
    "\n",
    "Let's say we want to run nginx container and keep the permanent copy of log files generated during it's run for later analysis. Nginx log files are generated at /var/log/nginx by default and it is not accessible from host system.\n",
    "\n",
    "__Step 1: Bindmount Volume__:\n",
    "\n",
    "`docker run --name=nginx -d -v ~/nginx_logs:/var/logs/nginx -p 5000:80 nginx`\n",
    "\n",
    "* `--name=nginx` names the container so that we can refer it easily\n",
    "* `-d` run the container in detached state i.e in background so that we can have access to the terminal from where we are running the docker run command\n",
    "* `-v ~/nginx_logs:/var/logs/nginx` bindmounts volume that links `/var/logs/nginx` directory from inside the container to `~/nginx_logs` directory of the host. Docker uses __`:`__ to split host path with container path and __host path always comes first__. \n",
    "* `-p 5000:80` __port forwarding__. This flag maps container's port 80 to port 5000 of the host machine.\n",
    "* `nginx` name of the image\n",
    "\n",
    "> The -v flag is very flexible. It can bindmount or name a volume with just a slight adjustment in syntax. If the first argument begins with a / or ~/, you’re creating a bindmount. Remove that, and you’re naming the volume.\n",
    "For more details: https://www.digitalocean.com/community/tutorials/how-to-share-data-between-docker-containers\n",
    "\n",
    "__Step 2: Access Data on Host__:\n",
    "Just go to the directory `~/nginx_logs` and see the log file.\n",
    "> If you make any changes to the `~/nginx_logs` folder, you’ll be able to see them from inside the Docker container in real time as well.\n",
    "\n",
    "> Multiple containers can share the same bind mount.\n",
    "\n",
    "## Named Volumes\n",
    "\n",
    "- They are more recent ways of creating volumes and they exist outside the container lifecycle.\n",
    "- Named volumes support more feature thatn bindmount such as remote cloud storage\n",
    "\n",
    "```docker\n",
    "docker create volume my_volume\n",
    "docker run -v my_volume:/directory/in/container ...\n",
    "```\n",
    "\n",
    "## Docker Memory\n",
    "\n",
    "Docker for Windows and Mac comes with gui where we can increase the maximum memory allocated to Docker. This is because the docker engine runs on top of VM that allocated the default memory to Docker. \n",
    "\n",
    "In case of Linux, docker has the whole machine for its use as it doesn't need to run on VM. So, if you have to assigne >5GB (let's say) to your container then you don't need to do anything for linux dockers. Instead in linux dockers you can limit how much memory a container can use with memory flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker Syntax\n",
    "\n",
    "EXAMPLE 1:\n",
    "```python\n",
    "FROM node:10.9.0\n",
    "\n",
    "COPY . .\n",
    "\n",
    "RUN npm install\n",
    "\n",
    "EXPOSE 8080\n",
    "\n",
    "CMD npm start\n",
    "\n",
    "# EXPLANATION:\n",
    "\n",
    "# FROM it's better to use version rather than latest as the latest will override the current working docker image and if the latest image has problem then it's tricky to roll back. Versioning gives more control\n",
    "\n",
    "# COPY current directory to working directory in container. Here the working dir of container will inherit from node image.\n",
    "# ADD is more advanced/powerful than COPY. For e.g it automatically extracts the archive and it has support for URLs. Use COPY by default to avoid any surprises unless you specifically need it.\n",
    "\n",
    "# RUN will run the shell command in the current workind directory\n",
    "\n",
    "# EXPOSE tell what port the container should be listening on. It's merely for documentation as it does not actually publish the port number on the host machine. To publish the ports and enable the client to connect to them, we need to use `-p` flag from the command line \n",
    "\n",
    "# CMD tells what command to run when someone starts the container. The above cmd tells docker to run node server for our application on start.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "__WORKDIR__: By default the workdir is set to `/`\n",
    "```\n",
    "FROM ubuntu:latest\n",
    "WORKDIR var\n",
    "WORKDIR log/nginx\n",
    "CMD pwd\n",
    "```\n",
    "---\n",
    "__ADD__ VS __COPY__:\n",
    "\n",
    "- ADD is more advanced than COPY in two ways - 1. It extracts tarball 2. Can use remote URL for download\n",
    "- Docker recommends use of COPY unless it's a remote file.\n",
    "---\n",
    "__RUN__ has 2 forms:\n",
    "\n",
    "`RUN <command> (known as the shell form)`\n",
    "\n",
    "`RUN [\"executable\", \"parameter 1\", \" parameter 2\"] (known as the exec form)`\n",
    "\n",
    "e.g of shell form\n",
    "\n",
    "```RUN echo `uname -rv` > $HOME/kernel-info```\n",
    "\n",
    "**Very important point to note about _RUN_ is that docker uses the command string/instruction of RUN for cache and not the actual contents of the RUN instruction.** For e.g conside the RUN instruction below\n",
    "\n",
    "```\n",
    "FROM ubuntu:16.04\n",
    "RUN apt-get update\n",
    "```\n",
    "Docker will cache all layers, however\n",
    "```\n",
    "FROM ubuntu:18.04\n",
    "RUN apt-get update\n",
    "```\n",
    "In this case, Docker reuses the cache of the previous image and, as a result, the image build can contain outdated packages.The cache for the RUN instructions can be invalidated by using the --no-cache flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`docker build -t <image_name> -f <dockerfile_path` build_context_path\n",
    "\n",
    "> if you don't provide -t flag docker will give random name; if you don't provide -f flag docker will consider dockerfile to be present in the current dir; build_context_path is usually \".\" but if your context is somewhere else then give the relative path from current dir\n",
    "\n",
    "The docker build command builds Docker images from a Dockerfile and a “context”. A build’s context is the set of files located in the specified PATH or URL. The build process can refer to any of the files in the context. For example, your build can use a COPY instruction to reference a file in the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bash\n",
    "\n",
    "1. find [where_to_find] -name [what_to_find]\n",
    "\n",
    "e.g `find / -name *.whl`\n",
    "\n",
    "---\n",
    "\n",
    "2. To check exposed ports e.g when aws instance is launched and we want to check from terminal\n",
    "\n",
    "`netstat -ltpn`\n",
    "\n",
    "---\n",
    "\n",
    "3. CHECKING MEMORY and DISK USAGE\n",
    "\n",
    "`free -m` - available and used RAM and swap\n",
    "\n",
    "`du -sh *` - disk usage for all files and folders (*) in current directory\n",
    "\n",
    "`df -h` - disk space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GetIntoDevOps Notes\n",
    "\n",
    "### CI\n",
    "\n",
    "- Emphasises automated testing to ensure the new code changes work as intended and does not break anything (fast feedback)\n",
    "- Pipeline automation servers (like Jenkins) are used to implement automatic testing\n",
    "\n",
    "### CD (Continuous Delivery)\n",
    "\n",
    "- While CI is the act of merging code as fast as possible, CD is the act of shipping changes to production frequently, in small increments\n",
    "- In practice, the code in main branch should be deployable to production at all times\n",
    "\n",
    "### CD (Continuous Deployment)\n",
    "\n",
    "- While Delivery makes sure everything in the main branch should be in deployable state, the actual automation of deployment without human intervention is part of this stage.\n",
    "\n",
    "### A/B Testing\n",
    "\n",
    "- Introducing two changes in the application and measuring which variant works better\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Journey Questions\n",
    "\n",
    "1. Basic pipeline running for various repos\n",
    "2. Use docker\n",
    "3. Implement bazel caching\n",
    "4. Implement docker caching\n",
    "5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VSCode\n",
    "\n",
    "Code quality tools currently using:\n",
    "1. Flake8\n",
    "2. mypy\n",
    "3. black\n",
    "4. isort\n",
    "\n",
    "For python dependency and management:\n",
    "\n",
    "`pipenv`\n",
    "\n",
    "All the above tools have been installed globally.\n",
    "\n",
    "My overall json settings as of now\n",
    "```json\n",
    "{\n",
    "    \"python.linting.pylintEnabled\": false,\n",
    "    \"python.linting.mypyEnabled\": true,\n",
    "    \"python.linting.enabled\": true,\n",
    "    \"python.linting.flake8Enabled\": true,\n",
    "    \"editor.formatOnSave\": true,\n",
    "    \"python.formatting.provider\": \"black\",\n",
    "    \"python.formatting.blackArgs\": [\n",
    "        \"--line-length=120\"\n",
    "    ],\n",
    "    \"python.linting.flake8Args\": [\n",
    "        \"--max-line-length=120\",\n",
    "        \"--ignore=E402\",\n",
    "    ],\n",
    "    \"[python]\": {\n",
    "        \"editor.codeActionsOnSave\": {\n",
    "            \"source.organizeImports\": true\n",
    "        }\n",
    "    },\n",
    "    \"window.zoomLevel\": 2,\n",
    "    \"files.insertFinalNewline\": true,\n",
    "}\n",
    "```\n",
    "\n",
    "If I have to exclude my global settings let's say for other projects then I'll go into that virtual environment and use that python interpreter (ctrl+shift+p and then select python interpreter corresponding to that env). Thus the vscode settings.json would be different for that environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to increase EBS Volume size without downtime\n",
    "\n",
    "__Phase 1__\n",
    "Increasing the size from console\n",
    "\n",
    "1. Select Volume that you want to scale\n",
    "2. Create snapshot (Optional)\n",
    "3. Click on Modify Volume\n",
    "4. Give the size\n",
    "> let's say it was 30GB and you want to make it 50GB, then give 50GB in the entry field\n",
    "5. Click on Modify\n",
    "\n",
    "__Phase 2__\n",
    "Now we extend the partition\n",
    "\n",
    "1. Type `lsblk` to list the block devices\n",
    "\n",
    "A Nitro based instance will show something like below\n",
    "```bash\n",
    "    $ lsblk\n",
    "\n",
    "    NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT\n",
    "    nvme0n1 259:0 0 600G 0 disk\n",
    "    └─nvme0n1p1 259:1 0 400G 0 part /\n",
    "```\n",
    "\n",
    "T2 based instance will show something like below\n",
    "```bash\n",
    "\n",
    "\n",
    "    $ lsblk\n",
    "\n",
    "    NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT\n",
    "    xvda 202:0 0 600G 0 disk\n",
    "    └─xvda1 202:1 0 400G 0 part /\n",
    "\n",
    "```\n",
    "\n",
    "2.1 __Extending Partition on Nitro based instance__\n",
    "\n",
    "The root volume, /dev/nvme0n1, has a partition, /dev/nvme0n1p1. \n",
    "Notice that the size of the root volume reflects the new size but the size of the partition reflects the original size which must be extended before you can extend the file system.\n",
    "\n",
    "To extend the partition on the root volume, use the following `growpart` command\n",
    "```bash\n",
    "$ sudo growpart /dev/nvme0n1 1\n",
    "```\n",
    "\n",
    "> Note that /dev/nvme0n1 is the name of the root volume and 1 is the partition number\n",
    "\n",
    "2.2 __Extending Partition on T2 based instance__\n",
    "\n",
    "The root volume, /dev/xvda, has a partition, /dev/xvda1. \n",
    "Notice that the size of the root volume reflects the new size but the size of the partition reflects the original size which must be extended before you can extend the file system.\n",
    "\n",
    "To extend the partition on each volume, use the following growpart commands.\n",
    "```bash\n",
    "$ sudo growpart /dev/xvda 1\n",
    "```\n",
    "\n",
    "3. __Extend the file system itself__\n",
    "\n",
    "Based on the type of instance use either\n",
    "```bash\n",
    "sudo resize2fs nvme0n1p1\n",
    "```\n",
    "or \n",
    "```bash\n",
    "sudo resize2fs /dev/xvda1\n",
    "```\n",
    "> last argument is the partition name that you see when you use lsblk\n",
    "\n",
    "You can now verify the size by `df -h`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
