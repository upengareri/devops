{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every git commit needs user and email, so configure your git with -\n",
    "\n",
    "```\n",
    "git config --global user.name \"abc\"\n",
    "git config --global user.email \"xyz@blah.com\"\n",
    "```\n",
    "\n",
    "To check your settings:\n",
    "`git config --list`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IAM: User, Group, Role and Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main actors in IAM are users, groups, roles and policies. And what you need to understand about AWS and never forget is that\n",
    "\n",
    "> Everything in AWS is an API\n",
    "\n",
    "And to execute any API or any of its methods, first we have to authenticate and then authorize that particular user/group/role.\n",
    "\n",
    "Ex: An operator wants to put an object to a S3 bucket. This process happens through a set of API calls within AWS. Basically we call the S3 API and a method of it to put the object into the particular bucket (say method `put_object_in_s3`). For that we may want to provide the name of the bucket, the object, and most importantly we need to provide set of credentials (username with password or secret key or etc) in order to tell the AWS API Engine who this user/group/role is.\n",
    "\n",
    "The first thing API Engine does is, look at those credentials sent with the API. Then it validate those (whether they are correct, active) credentials indicating that this request is coming from a actual valid user, group or role. Then what the API Engine does is (as it now knows who sent this API request) it takes the policy documents associated with the particular operator (user or role) and evaluate them as a single view. That is we check whether the action called in the API is authorized for that operator.\n",
    "\n",
    "__IAM user__ - In the context of IAM, an user is a “permanent” named operator (human or machine). What’s important to note is that it’s credentials (credentials maybe username password or access key or a secret key) are permanent and stays with that named user. So by that AWS knows that what are the authentication methods (username password authentication method or secret key method or etc) for this user (as its permanent and stays with the user).\n",
    "\n",
    "__IAM group__ - As in the above image, a group is a collection of users. And note that a user can be in many groups as well.\n",
    "\n",
    "__IAM roles__ - Roles are not Permissions !!!. A role is also an authentication method just as IAM users and groups. As an user, a role is also a operator (could be a human, could be a machine). Difference is that credentials with roles are temporary.\n",
    "\n",
    "__Policy Documents__ - As stated earlier, roles are not Permissions. Permissions in AWS are completely handled by objects called `Policy Documents`. Policy Documents are JSON documents. Policy Documents can directly be attached to Users, Groups or Roles. When a policy document gets attached to any of above operator, then only they get permissions do stuff. A policy document lists things like: Specific API or wildcard group of APIs that gets whitelisted against which resources, and Conditions for those API executions (like allow only if this user, group or role in the home network or allow from any location, allow only at certain times of day and etc)\n",
    "\n",
    "> Last but not least, Authentication in AWS is done via (IAM users, groups and roles) whereas Authorization is done by Policies.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between Region, Availability Zone (AZ) and Edge Location\n",
    "\n",
    "- A region is a __physical location in the world__ consisting to 2 or more AZs\n",
    "- An AZ is one or more discrete __data centers__, each with redundant power, networking and connectivity, housed in separate facilities\n",
    "- Edge Locations are end points for AWS which are used for __caching content__. Typically, this consists of CloudFront, Amazon's Content Delivery Network (CDN)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A __Virtual Private Cloud (VPC)__ is a virtual network dedicated to a single AWS account. It is logically isolated from other virtual networks in the AWS cloud, providing compute resources with security and robust networking functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  #### #of Edge Locations > # of Availability Zones > # of Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the important services: Storage, Compute, Database, Network and Security, Identity and Roles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Whenever we are doing anything with IAM the region is set to Global.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IAM\n",
    "\n",
    "- It is universal\n",
    "- ROOT account has GOD mode. Make MFA (Multi Factor Authentication) for root user.\n",
    "\n",
    "- **USER**\n",
    "    - Two types of access:\n",
    "        1. Programmatic access (access key ID and secret access key)\n",
    "        2. Console access (password)\n",
    "    - At the beginning, user gets no permission/authorization. We need to give permission via policy document. For example, we can give password change related policy (`IAMUserChangePassword`) to the user so that it can change its password.\n",
    "    - We can add the user to group(s). Group can have different permissions/policies assigned and those policies are automatically inherited to the user under that group.\n",
    "    - Set __password policy__ for the user in account settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BILLING ALARM (CLOUD WATCH)\n",
    "\n",
    "We can use SNS (SIMPLE NOTIFICATION SERVICE) under Cloud Watch to automatically send us the notification if the bill exceeds the threshold set by us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 (Simple Storage Service)\n",
    "\n",
    "- It is **object-based** i.e allows you to upload files\n",
    "- Files can be from 0 to 5 TB\n",
    "- There is unlimited storage\n",
    "- Files are stored in **buckets** (folders)\n",
    "- S3 is a universal namespace (names must be unique globally). The region changes to `Global` when on S3 similar to IAM\n",
    "- http 200 code if the upload was successful\n",
    "\n",
    "**s3 objects** consists of following -\n",
    "* key (name of the object)\n",
    "* value (this is simply the data and is made up of bytes)\n",
    "* version id (important for versioning)\n",
    "* metadata (info about the data)\n",
    "* subresources\n",
    "    * access control lists (access/permissions (e.g lock an object) on the bucket level as well as individual object level)\n",
    "    * torrent\n",
    "    \n",
    "**s3 data consistency model**\n",
    "* Read after write consistency for PUTS of new objects (as soon as you create, you'll be able to read that object)\n",
    "* Eventual consistency for overwrite PUTS and DELETES (might take some time to reflect the change)\n",
    "\n",
    "**s3 storage classes or access tier**\n",
    "1. s3 standard (4 9s availability i.e 99.99% availability; 11 9s durability)\n",
    "2. s3 IA (infrequency accessed) e.g if we access something at the end of every month\n",
    "    - Lower fee but retrieval charge\n",
    "3. s3 One Zone IA (similar to deprecated RRS i.e reduced redundancy service) \n",
    "    - We do not care about losing the data if something happens as only 1 AZ\n",
    "4. s3 Intelligent Tiering\n",
    "    - Uses machine learning to analyse data usage and automatically moves data to most cost effective access tier or storage class to reduce cost\n",
    "5. s3 glacier\n",
    "    - for data archival\n",
    "    - we can configure retrieval time (between minutes and hours)\n",
    "6. s3 glacier deep archive\n",
    "    - lowest cost storage class (which can go upto 12 hours for data retrieval)\n",
    "    \n",
    "> Note: Read s3 FAQs as s3 is very important for exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3 storage expense wise\n",
    "\n",
    "s3 standard > s3 IA > s3 IT > s3 one zone IA > s3 glacier > s3 glacier deep archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 basics\n",
    "\n",
    "#### Access control\n",
    "To setup **access control** to S3 we can use -\n",
    "1. Bucket Policy - works on bucket level\n",
    "2. Access Control List (ACL) - works on individual object level\n",
    "\n",
    "We can configure s3 bucket to log access requests. This log can be sent to another bucket and even another bucket in different AWS account.\n",
    "\n",
    "#### Encryption\n",
    "- `Encryption in transit` (HTTPS encryption) is achieved by\n",
    "    - SSL/TLS\n",
    "    \n",
    "- `Encryption at rest (Server Side)` is achieved by\n",
    "    - S3 Managed Keys (__SSE-S3__: Server Side Encryption): here Amazon manages the key\n",
    "    - AWS Key Management Service (__SSE-KMS__): here we and Amazon together manage the keys\n",
    "    - Server Side Encryption with Customer Provided Keys (__SSE-C__): here we give Amazon our own keys\n",
    "    \n",
    "- `Encryption at rest (Client Side)`\n",
    "    - where we encrypt the object at our end to put it in S3\n",
    "    \n",
    "#### Versioning S3\n",
    "- Great backup tool\n",
    "- Once enabled on bucket, cannot be disabled but can only be suspended.\n",
    "- For deletion of bucket, we can enable MFA (multi factor authentication)\n",
    "- Integrates with Lifecycle rules\n",
    "\n",
    "SCENARIOS:\n",
    "\n",
    "> If we upload the same file again with some changes, the file gets overwritten and the permission (if you have made the previous uploaded file public) is private (default behaviour). So, we need to make it public again. The first version will still be public and can be seen under \"Version Show\" button. And when we delete the object, we can still see the different versions and the latest one would have \"delete marker\" on it. If we delete the \"delete marker\" object from version table, then it gets removed from the stack and the second last gets to top and becomes the latest revision.\n",
    "\n",
    "> On overriding the same file, size of it increases due to version control. So keep in mind if you are updating huge file then the size will increase exponentially. In that case, you might want to look into Lifecyle rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lifecycle Rule\n",
    "\n",
    "We can create lifecycle rule configuration that is applicable for whole bucket or for specific tags corresponding to objects within the bucket.\n",
    "\n",
    "There are two types of actions we can take on bucket/objects:\n",
    "\n",
    "1. Automatic **transition** to tiered storage (For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after you created them, or archive objects to the S3 Glacier storage class one year after creating them).\n",
    "2. **Expire** your objects (Define when objects expire. Amazon S3 deletes expired objects on your behalf).\n",
    "\n",
    "> Both the actions gets triggered (once enabled) after \"N\" number of days of the object creation.\n",
    "\n",
    "We use lifecycle rule when - \n",
    "\n",
    "1. If you upload periodic logs to a bucket, your application might need them for a week or a month. After that, you might want to delete them.\n",
    "\n",
    "2. Some documents are frequently accessed for a limited period of time. After that, they are infrequently accessed. At some point, you might not need real-time access to them, but your organization or regulations might require you to archive them for a specific period. After that, you can delete them.\n",
    "\n",
    "3. You might upload some types of data to Amazon S3 primarily for archival purposes. For example, you might archive digital media, financial and healthcare records, raw genomics sequence data, long-term database backups, and data that must be retained for regulatory compliance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object LOCK (Locking objects for future edits/delete for regulations/ critical data etc.)\n",
    "\n",
    "- We can enable `Object Lock` only when **VERSIONING** in enabled for a bucket.\n",
    "- We use Object Lock to enable WORM mode i.e Write Once Read Many. This is done to avoid people from overwriting or deleting version of an object. So you put one version and don't want it to be changed or deleted for some time (retention period or indefinitely with legal hold)\n",
    "- There's lot of combinations that can be put on Object Lock and to read about those check https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock-overview.html which has a nice documentation.\n",
    "- Similary, we have Glacier Vault Lock Policy for WORM mode and once locked, the policy cannot be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3 PERFORMANCE\n",
    "\n",
    "1. **PREFIX**: Bare in mind while designing your architecture/bucket that more prefix you have the better performance we achieve.\n",
    "   Example of prefix: mybucketname/folder1/subfolder1/abc.jpg > **/folder1/subfolder1**\n",
    "2. **SSE-KMS**: S3 performance apart from prefix depends on **SSE-KMS**. So if our bucket/object uses that encryption then it will use KMS quota which depending on region can be 5500, 10000 or 30000 requests per second. This is because for example when we upload a file, we will call `GenerateDataKey` in the KMS API and similary while downloading the `Decrypt` key.\n",
    "3. **MULTIPART UPLOADS**: Recommended for files > 100MB and necessary for files > 5GB\n",
    "4. **BYTE RANGE FETCHES**: Parallelize byte range downloads\n",
    "\n",
    "> See Exam Tips of Chapter 18 for # of request per prefix and for SSE-KMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3 SELECT and GLACIER SELECT\n",
    "\n",
    "- to retrieve only subset of data. For example if we have a zip file containing CSV files and we want to get only one file then instead of **fetching** the entire zip, **decompressing** it and then reading the actual file, we can use S3 select to query just that file. It will save money for data transfer as well as increase speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHARE BUCKET\n",
    "\n",
    "There are 3 different ways to share S3 buckets -\n",
    "\n",
    "1. Using Bucket Policy and IAM (bucket level): Programmatic access only\n",
    "2. Using Bucket ACLs and IAM (object level): Programmatic access only\n",
    "3. Cross account IAM Roles: Console and Programmatic access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CROSS REGION REPLICATION (CRR) OF S3 BUCKET (put in one bucket; automatically in another as well)\n",
    "\n",
    "* Not that important (understand just the high level)\n",
    "* Versioning must be enabled on the source bucket for CRR\n",
    "* Files already existing in the source bucket are automatically replicated\n",
    "* All subsequent updates will be automatically updated\n",
    "* Delete markers and delete individual versions are not replicated\n",
    "* Changing access of objects from private to public in source bucket doesn't impact the object in destination bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRANSFER ACCELERATION (to transfer files faster)\n",
    "\n",
    "Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.\n",
    "\n",
    "> When using Transfer Acceleration, additional data transfer charges may apply.\n",
    "\n",
    "Why you might want it?\n",
    "\n",
    "* You have customers that upload to a centralized bucket from all over the world.\n",
    "* You transfer gigabytes to terabytes of data on a regular basis across continents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA SYNC (like between on-premise server and AWS)\n",
    "\n",
    "* Used to move large amounts of data from on-premise data center to AWS and vice versa\n",
    "* Need datasync agent on the source to transfer data\n",
    "* Used with **NFS** and **SMB** compatible file system\n",
    "* Replication can be done hourly, daily or weekly\n",
    "* Can be used to replicate EFS to EFS as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLOUD FRONT (for delivering content)\n",
    "\n",
    "* CloudFront is basically a CONTENT DELIVERY NETWORK (CDN)\n",
    "* CDN is a system of distributed servers that delivers content to a user based on location etc.\n",
    "* KEY TERMINOLOGIES:\n",
    "    - ORIGIN: This is the origin of all files that CDN will distribute. This can be either S3 bucket, EC2 instance, Elastic Load Balancer, or Route53\n",
    "    - DISTRIBUTION: It's the name (domain name) given to CDN which is a collection of edge locations\n",
    "    - Web Distributino: Typically used for websites\n",
    "    - RTMP (Real Time Messaging Protocol): For Media Streaming\n",
    "\n",
    "> Edge Locations are not just READ only. We can write them to (i.e put objects on them). This we saw in Transfer Acceleration.\n",
    "\n",
    "> Objects are cached for __TTL__ (Time To Live) value.\n",
    "\n",
    "> You can clear cached object but that will be charged. Example, if we have uploaded something and the users are still getting the old video rather than the new one, then you can go in and probably clear those cached data.\n",
    "\n",
    "Doc: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\n",
    "\n",
    "#### CLOUD FRONT SIGNED URLs and COOKIES (e.g for premium users)\n",
    "\n",
    "* Use signed URLs/Cookies when you want to secure content so that only people who authorize can use it.\n",
    "* Signed URL is for single file. **1 file = 1 URL**\n",
    "* Signed Cookie is for multiple files. **1 cookie = multiple files**\n",
    "* If your origin is EC2, use CloudFront. If your origin is S3 and you have single file for user when you can use **S3 signed URL**\n",
    "\n",
    "#### SNOWBALL (Migration of huge data)\n",
    "* Import to S3, export to S3. Your requirement for snowball can depend on the size of file as well as available internet connection. For example if you have 2TB data with 44Mbps connection then you migth use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storage Gateway (move backups to the cloud)\n",
    "\n",
    "Scroll to the bottom in the below URL for better info.\n",
    "\n",
    "Doc: https://aws.amazon.com/storagegateway/?nc=sn&loc=0&whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ATHENA vs MACIE\n",
    "\n",
    "ATHENA:\n",
    "- It is an interactive query service i.e allows you to query on data stored in S3\n",
    "- It is serverless\n",
    "- Commonly used to **analyse log data** stored in S3\n",
    "\n",
    "MACIE:\n",
    "- It is a security service that uses AI to analyze data stored in S3 and helps **identify PII** (Personal Identifiable Information)\n",
    "- Can also be used to analyse CloudTrail logs data for suspicious activity\n",
    "- Includes dashboard, report and alerting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _TIP: READ S3 FAQs in aws as S3 is really really an important topic for Associate Exams_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUIZ ON S3\n",
    "\n",
    "1. *Power user access allows access to all AWS services except management of users and groups within IAM*\n",
    "\n",
    "2. You are a solutions architect working for a large engineering company that are moving from a legacy infrastructure to AWS. You have configured the company's first AWS account and you have set up IAM. Your company is based in Andorra, but there will be a small subsidiary operating out of South Korea, so that office will need its own AWS environment. Which of the following statements is true?\n",
    "    - Correct Answer: You will need to configure **Users and Policy Documents only once**, as they are applied globally. (Here remember that IAM is GLOBAL)\n",
    "\n",
    "3. You have created a new AWS account for your company, and you have also configured multi-factor authentication on the root account. You are about to create your new users. What strategy should you consider in order to ensure that there is good security on this account.\n",
    "    - Correct Answer: Enact a **strong password policy**. user passwords must be changed every 45  days, with each password containing a combination of uppercase, numbers, special characters.\n",
    "    \n",
    "4. You have been asked to advise on a scaling concern. The client has an elegant solution that works well. As the information base grows they use CloudFormation to spin up another stack made up of an S3 bucket and supporting compute instances. The trigger for creating a new stack is when the PUT rate approaches 100 PUTs per second. The problem is that as the business grows that number of buckets is growing into the hundreds and will soon be in the thousands. You have been asked what can be done to reduce the number of buckets without changing the basic architecture.\n",
    "    - Correct Answer: Change the trigger level to around 3500 PUTS as S3 can now accommodate much higher PUT and GET levels.\n",
    "    - Explanation: Until 2018 there was a hard limit on S3 puts of 100 PUTs per second. To achieve this care needed to be taken with the structure of the name Key to ensure parallel processing. As of July 2018 the limit was raised to 3500 and the need for the Key design was basically eliminated. Disk IOPS is not the issue with the problem. The account limit is not the issue with the problem.\n",
    "    \n",
    "5. You run a meme creation website where users can create memes and then download them for use on their own sites. The original images are stored in S3 and each meme's metadata in DynamoDB. You need to decide upon a low-cost storage option for the memes, themselves. If a meme object is unavailable or lost, a Lambda function will automatically recreate it using the original file from S3 and the metadata from DynamoDB. Which storage solution should you use to store the non-critical, easily reproducible memes in the most cost-effective way?\n",
    "    - Correct Answer: S3-1Z-IA\n",
    "    - Explanation: S3 – OneZone-IA is the recommended storage for when you want cheaper storage for infrequently accessed objects. It has the same durability but less availability. There can be cost implications if you use it frequently or use it for short lived storage. Glacier is cheaper, but has a long retrieval time. RRS has effectively been deprecated. It still exists but is not a service that AWS want to sell anymore.\n",
    "    \n",
    "6. What is the availability of S3 – OneZone-IA?\n",
    "    - Correct Answer: 99.50%\n",
    "    - Explanation: OneZone-IA is only stored in one Zone. While it has the same Durability, it may be less Available than normal S3 or S3-IA\n",
    "    \n",
    "7. One of your users is trying to upload a 7.5GB file to S3. However, they keep getting the following error message: \"Your proposed upload exceeds the maximum allowed object size.\". What solution to this problem does AWS recommend?\n",
    "    - Correct Answer: Design your application to use Multi-part upload API for all objects.\n",
    "    - Explanation: multipart recommended for more than 100 mb and necessary for 5GB or more. Also, 5 TB max upload size for a file is to be remembered.\n",
    "    \n",
    "8. AWS S3 has four different URLs styles that it can be used to access content in S3.  The Virtual Hosted Style URL, the Path-Style Access URL, the Static web site URL, and the Legacy Global Endpoint URL.  Which of these represents a correct formatting of the  Virtual Hosted Style URL  style\n",
    "    - Correct Answer: https://bucket-name.s3.Region.amazonaws.com/abc.png\n",
    "    - Explanation:\n",
    "        Virtual Hosted: https://bucket-name.s3.Region.amazonaws.com/key_name\n",
    "        \n",
    "        Path Style: https://s3.Region.amazonaws.com/bucket-name/key_name\n",
    "        \n",
    "        Virtual style puts your bucket name 1st, s3 2nd, and the region 3rd.\n",
    "        \n",
    "        Path style puts s3 1st and your bucket as a sub domain.\n",
    "        \n",
    "        Legacy Global endpoint has no region.\n",
    "        \n",
    "        S3 static hosting can be your own domain or your bucket name 1st, s3-website 2nd, followed by the region.\n",
    "        \n",
    "        AWS are in the process of phasing out Path style, and support for Legacy Global Endpoint format is limited and discouraged. However it is still useful to be able to recognize them should they show up in logs. https://docs.aws.amazon.com/AmazonS3/latest/dev/VirtualHosting.html\n",
    "        \n",
    "9. How many S3 buckets can I have per account by default?\n",
    "    - Correct Answer: 100\n",
    "    \n",
    "10. What is the availability of objects stored in S3?\n",
    "    - Correct Answer: 99.99%\n",
    "\n",
    "11. What is AWS Storage Gateway?\n",
    "    - Correct Answer: It is a physical or virtual appliance that can be used to cache S3 locally at customer's site.\n",
    "    - Explanation: At its heart it is a way of using AWS S3 managed storage to supplement on-premise storage. It can also be used within a VPC in a similar way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It's a virtual machine in the cloud.\n",
    "\n",
    "### EC2 Pricing Types\n",
    "1. **On-Demand**: for hourly or based on seconds (popular with developers)\n",
    "\n",
    "2. **Reserved**: based on contract for 1 year or 3 years terms\n",
    "\n",
    "    2.1 __Standard Reserved Instance__: After you select instance type, you cannot change it later.\n",
    "    \n",
    "    2.2 __Convertible Reserved Instance__: You can let's say move from t2 to r4 instance type in between the contract.\n",
    "    \n",
    "    2.3 __Scheduled Reserved Instance__: Let's say you run a school and you want the instances only between 9 and 10 when the students mostly login, then this type of ec2 is best for you.\n",
    "\n",
    "\n",
    "3. **Spot**: it's like bidding for ec2. If you bid at that price, you'll have your instance(s). Depends on Amazon's own supply and demand. Useful for applications with flexible start and end time.\n",
    "    > If Spot instance is terminated by Amazon (if they need that instance) then we will not be charged for partial hour of usage. However if we terminate by ourselves then we need to pay for the hour for which instance ran. \n",
    "\n",
    "4. **Dedicated Host**: dedicated machine for us and we can also pay on demand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EC2 instance type Mnemonic:\n",
    "FIGHT DR. MC. PXZ (in) AU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Points on ec2\n",
    "\n",
    "- __NAME__: EC2 inherits its name from the Tag\n",
    "- __SUBNET__: We'll discuss this later in more detail but it decides the __AZ of ec2 instances__ within that region and the __range of IPs__ that can be assigned to ec2 instances in that subnet. Subnets fall under VPC.\n",
    "- __SG__: Security group is like a virtual firewall to __control traffic__; basically deals with which __ip addresses__ can access the instance and through which __ports__.\n",
    "- __ENCRYPTION (KEY PAIR)__: To connect to ec2 instance, aws provides __asymmetric encryption__. To make it easy to understand, think of asymmetric as __key pair__. Symmetric is just one key. You can consider asymmetric as you have pad lock (public key) with a key. So, basically when you lock a bike with padlock, any public can see that padlock (public key is exposed to public) but only you have the key (private key) to open that padlock. People can try to open your padlock but the handshake won't happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Security Group\n",
    "\n",
    "* We cannot blacklist anything in SG but we can in NACL. For e.g we cannot configure in SG to block HTTP or SSH or certain IP address but these things can be done in NACL which we'll see in Networking section in more detail. But in SG everything is blocked by default, so we allow ports or IPs (this is different because we are whitelisting but we cannot blacklist as everything is blacklisted by default in the beginning).\n",
    "* SG are stateful while NACL are stateless. This means whatever we create in \"Inbound\" in SG are automatically set for \"Outbound\" as well but in NACL we have to set separately for \"Inbound\" as well as \"Outbound\".\n",
    "* We can add multiple SGs to an instance. So for e.g if we have created 1 SG that only allows HTTP port and 1 SG that allows SSH and MySQL, and we have created an instance that needs all HTTP, SSH and MySQL then we add those SGs to our instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EBS\n",
    "\n",
    "__Replication__\n",
    "- Each volume is automatically replicated within AZ to protect us from component failure.\n",
    "\n",
    "### TYPES \n",
    "(based on VOLUME and IOPS; Space is cheap)\n",
    "\n",
    "__SSD__\n",
    "\n",
    "1. __General Purpose SSD__: balances price and performance; Volume of 1gb - 16tb with 16k iops\n",
    "2. __Provisioned IOPS SSD__: high performance; for use-case that requires high storage and fast input/output operation like in case of database. Vol: 4gb-16tb with 64k iops\n",
    "\n",
    "__HDD__: they give high storage compared to SSD but less iops\n",
    "\n",
    "3. __Throughput Optimized HDD__: for high storage but less iops use case. Vol: 500gb-16tb with 500 iops\n",
    "4. __Cold HDD__: cheapest; for infrequently accessed high storage but very less iops. Vol: 500gb-16tb with 250 iops\n",
    "5. __Magnetic__: previous generation HDD; if for some reason you don't want glacier. very infrequently accessed.\n",
    "\n",
    "AMIs can be used to create instance with either EBS as root volume or Instance Store as root volume. You can create instance with instance store as root and add ebs as extra later but you cannot create instance with ebs as root and then add instance store later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encrypted Root Device Volume and Snapshots\n",
    "\n",
    "Root device is where your OS is installed.\n",
    "\n",
    "How to encrypt an unencrypted volume?\n",
    "\n",
    "Create snapshot and then copy. While copying it asks for encryption again.\n",
    "Remember copy is different from copy image.\n",
    "\n",
    "Now we have encrypted copy of the volume as snapshot.\n",
    "\n",
    "What we can do with snapshot?\n",
    "\n",
    "We create snapshot of volume. We can use snapshot to create image (AMI) which can then be used to launch an instance.\n",
    "1. We can copy the snapshot.\n",
    "2. We can create ami\n",
    "3. We can create volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CloudWatch\n",
    "\n",
    "- Used for monitoring __performance__. Some of the examples it can monitor \n",
    "Compute:\n",
    "    - EC2, Autoscaling, ELB, Route53\n",
    "Storage and CDN:\n",
    "    - EBS, Storage gateway, CloudFront, S3\n",
    "    \n",
    "- It also monitors applications running on AWS\n",
    "- Cloudwatch with EC2 will monitor __every 5mins by default__ but can be set to __1mins__ with detailed monitoring\n",
    "- You can have cloudwatch alarms which triggers notifications\n",
    "\n",
    "> CloudTrail on the other hand is like a cctv that audits on API calls i.e who did what as a user. CloudWatch is about performance and cloudtrail is about auditing.\n",
    "\n",
    "To cover RAM utilization and disk usage, we need to create custom cloudwatch metrics.\n",
    "\n",
    "## EFS\n",
    "\n",
    "- It's a file storage system (popular for Linux)\n",
    "- We can attach multiple EC2 instances with EFS unlike EBS.\n",
    "- It can shrink and expand as you remove and add files.\n",
    "\n",
    "## FSx For Windows\n",
    "\n",
    "- It's basically a file server for Windows\n",
    "- It provides managed Windows file system so you can move your Windows based applications that requires file storage to AWS\n",
    "- It's different from EFS as it manages __SMB (Server Message Block)__ based file services and is designed for Windows applications\n",
    "- Amazon does not support EFS for EC2 instances running Windows\n",
    "\n",
    "## FSx for Lustre\n",
    "\n",
    "- When you need elastic storage but for HPC (High Performance Computing) applications, Big Data, ML and applications that require high throughput and sub low-level latencies.\n",
    "- It can store data directly on S3\n",
    "\n",
    "## Summary of EFS and FSx\n",
    "> EFS for Linux workloads\n",
    "\n",
    "> FSx for Windows for windows workloads\n",
    "\n",
    "> FSx For Lustre for high performance workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall: 4 Ways to achieve HPC for our business\n",
    "\n",
    "### 1. Data Transfer (To get our Data into AWS)\n",
    "1.1 Snowball and Snowmobile (terabytes/petabytes worth of data)\n",
    "\n",
    "1.2 AWS DataSync (puts an agent in on-premise system) to store data on S3, EFS, FSx for windows etc.\n",
    "\n",
    "1.3 Direct Connect (creates dedicated line or direct network connection between on-premise and AWS)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Compute and Networking\n",
    "2.1 EC2 instances that are CPU or GPU optimized\n",
    "\n",
    "2.2 EC2 fleets (Spot instances and Spot Fleets)\n",
    "\n",
    "2.3 Placement Group (putting EC2 instances in particular to __clustered PG__ to reduce latency)\n",
    "\n",
    "---\n",
    "\n",
    "2.4 Enhanced Networking: uses __SR-IOV__ (Single Root Input Output Virtualization) to provide higher PPS (packet per second) performance and reduced latency.\n",
    "    \n",
    "    2.4.1 ENA (10Gbps to 100Gbps) - Recommended\n",
    "    \n",
    "    2.4.2 Intel Virtual Function (VF) - For legacy instances (not recommended)\n",
    "\n",
    "2.5 Elastic Fabric Adapter (__EFA__): uses __OS by-pass__ which makes it much faster with much lower latency. Not supported for Windows yet (only Linux).\n",
    "\n",
    "\n",
    "### 3. Storage (all about IOPS)\n",
    "\n",
    "__Instance-attached__ Storage\n",
    "\n",
    "3.1 EBS: Scaled up to 64k IOPS with Provisioned IOPS type\n",
    "\n",
    "3.2 Instance Store: Scaled to millions of IOPS; low latency\n",
    "\n",
    "---\n",
    "\n",
    "__Network-attached__ Storage\n",
    "\n",
    "3.3 S3: object-based; not a filesystem\n",
    "\n",
    "3.3 EFS: Scale IOPS based on total size or use Provisioned IOPS\n",
    "\n",
    "3.4 FSx for Lustre: millions of IOPS, which is also backed by S3\n",
    "\n",
    "### 4. Orchestration and Automation\n",
    "\n",
    "4.1 AWS Batch\n",
    "\n",
    "4.2 AWS ParallelCluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WAF (Web Application Firewall) [Security Guard similar to NACL]\n",
    "\n",
    "- It lets us monitor HTTP(S) requests that are forwarded to CloudFront, ELB or API Gateway.\n",
    "- Its 3 behaviours:\n",
    "    - Allow all requests except the ones you specify\n",
    "    - Block all requests except the ones you specify\n",
    "    - Count the requests that match the properties you specify (passive mode so to say)\n",
    "\n",
    "> These behaviours can be related to Security Group that only permits us to allow things you specify otherwise everything is blocked by default in SG i.e we cannot block things in SG as customization\n",
    "\n",
    "- Some conditions that we can specify to WAF:\n",
    "    * IP address that the requests originate from\n",
    "    * Country that requests originate from\n",
    "    * Values in request headers e.g id and name in http://acloud.guru?id=1000&name=test\n",
    "    * Strings that appear in requests (using regex or specific string)\n",
    "    * Length of requests\n",
    "    * Presence of SQL code that is likely to be malicious (to prevent against SQL injection)\n",
    "    * Presence of script that is likely to be malicious (to prevent from cross-site scripting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databases 101\n",
    "\n",
    "RDS (Online Transaction Processing) __(OLTP)__ . Below are different flavors:\n",
    "1. SQL\n",
    "2. MySQL\n",
    "3. Oracle\n",
    "4. PostgreSQL\n",
    "5. Aurora\n",
    "6. MariaDB\n",
    "\n",
    "RDS has 2 key feature:\n",
    "1. Multi AZ (for disaster recovery) - allows you to have an exact copy in another AZ\n",
    "2. Replicas (for performance) (consider example of viral blog)\n",
    "\n",
    "Non Relational Database \n",
    "1. DynamoDB (No SQL)\n",
    "\n",
    "RedShift (Online Analytics Processing) __(OLAP)__ (for Business Intelligence and Data Warehousing)\n",
    "\n",
    "Elastic Cache (Amazon's Caching Solution) (for speed up performance of existing database)\n",
    "1. Memcached\n",
    "2. Redis\n",
    "\n",
    "---\n",
    "\n",
    "### RDS Backups\n",
    "- Automatic: done at scheduled maintenance window\n",
    "- Snapshot: done manually\n",
    "> Restoring backup would result in new RDS instance with new DNS endpoint\n",
    "\n",
    "### RDS Mulit AZ\n",
    "- Allows you to have an exact copy in another AZ\n",
    "- Not for improving performance\n",
    "- We can force failover from one AZ to another by rebooting RDS instance \n",
    "\n",
    "### RDS Read Replica\n",
    "- For incresing performance: allows read-only copy of production database (read-only as we can write to primary and the copy of primary get replicated to others)\n",
    "- Used for very read-heavy database workloads (like a viral blog)\n",
    "> to improve performance of RDS we use Read Replica and also Elastic Cache\n",
    "- Must have automatic backup turned on for read replica\n",
    "- upto 5 read replicas of any db\n",
    "- read replicas can have their own read replicas but with latency\n",
    "- read replicas will have their own DNS end point\n",
    "- read replicas can have multi AZ\n",
    "- read replica can be in separate region from primary db\n",
    "- read replicas can be upgraded to be their own databases; this breaks replication\n",
    "- can be for MySQL, Oracle, MariaDB, PostgreSQL, Aurora (not available for Microsoft SQL as of now)\n",
    "\n",
    "## DynamoDB\n",
    "\n",
    "- Stored on SSD (hence very fast)\n",
    "- Spread across 3 geographically distinct data centers (redundancy)\n",
    "- 2 different types of read models\n",
    "    1. Eventual Consistent Read (Default): if you write data to your dynamodb database you'll be able to see the update in a second\n",
    "    2. Strong Consistent Read: if you write data, you'll be able to see the update in less than a second\n",
    "> __1 second rule:__ If you have a requirement where you want to read the data before a second then you choose Strong Consistent Read else if in a second is fine then Eventual Consistent Read\n",
    "    \n",
    "## RedShift\n",
    "\n",
    "- petabyte scale data warehousing service\n",
    "- just like jenkins, redshit can be configured with _single node_ or _multi node where you have leader node (manage client connections) and compute node (store data and perform queries and computations)_. Upto 128 compute nodes.\n",
    "\n",
    "## DMS (Database Migration Service)\n",
    "\n",
    "- it's a cloud service that can be used to upload your database to cloud or off-load from cloud to on-premise or between combination of cloud and on-premise setups\n",
    "\n",
    "## Caching Service\n",
    "\n",
    "- The following services have caching capabilities\n",
    "    - CloudFront (caches your media files, videos, pics etc at edge locations near your end user)\n",
    "    - API Gateway\n",
    "    - Elatic Cache - consists of Memcached and Redis\n",
    "    - DynamoDB Accelerator (DAX)\n",
    "- Caching is a balancing act between __up-to-date, accurate information__ and __latency__\n",
    "\n",
    "![cahing](./resources/caching_services.png)\n",
    "\n",
    "The point of this diagram is to show that the more deeper you go with caching the more latency you might face. Here caching is possible in CloudFront level, API gateway level, lambda level, elastic cache level, and dynamodb level\n",
    "\n",
    "## EMR (Elastic Map Reduce)\n",
    "\n",
    "- It is industry leading cloud based platform for processing vast amount of data using open source tools such as Apache Spark. \n",
    "- The central component of EMR is cluster which has master node.\n",
    "- If it is multi-node cluster then it'll have master node, one or more core node(s) and optional task nodes\n",
    "- Master node manages cluster, stores the log, monitors the health of the cluster\n",
    "- Core node performs tasks and stores data in the Hadoop Distributed File System (HDFS) on your cluster\n",
    "- Task node just performs tasks and doesn't store data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Directory Service\n",
    "\n",
    "__AD Compatible__:\n",
    "- Managed Microsoft AD (aka Directory Service for Microsoft Active Directory)\n",
    "- AD Connector\n",
    "- Simple AD\n",
    "\n",
    "__Not AD Compatible__:\n",
    "- Cloud Directory\n",
    "- Cognito user pools\n",
    "\n",
    "## IAM Policies\n",
    "\n",
    "![ARN](./resources/arn.png)\n",
    "\n",
    "## Resource Access Manager (RAM)\n",
    "- Allows access of certain resources from one AWS account to another\n",
    "- For example, we can share Aurora DB to another aws account (say account2) by creating RAM in account1 and then accepting that request from account1 to account2 and then we can see the reflected db in account 2. We can now clone the aurora db that was shared to use it.\n",
    "\n",
    "## SSO (Single Sign On)\n",
    "- It helps centrall manage access to AWS accounts and business applications \n",
    "- Allows sign on to any SAML 2.0 enabled applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Route53\n",
    "- Route 53 is a global service\n",
    "- NS Records and SOA\n",
    "    - When we hit let's say abc.com, the top level domain server (in this case .com) will look for the IP associated with the domain name and it's gonna point to NS Record server. It'll then query the NS Record which will give us the Start of Authority (SOA) server which has the DNS record.\n",
    "    - DNS record consists of different things like \n",
    "        - __A-record (Address record)__ is used by computer to translate domain name to IP address.\n",
    "        - __CName (Canonical Name)__ to resolve one domain name to another. For e.g Batman will resolve to Bruce Wayne in yellow page phone address example\n",
    "        - __Alias record__ is similar to CName in that you can map one domain name to another. It is mainly used to __map resource record set in your hosted zone__ to ELBs, CloudFront distributions or S3 buckets that are configured as websites\n",
    "        > Difference b/w CName and Alias Record is that CName can't be used for naked domain names (zone apex record). For naked domain names, it must be either an A record or Alias name.\n",
    "\n",
    "Common DNS Types\n",
    "1. NS Records\n",
    "2. SOA Records\n",
    "3. A Records\n",
    "4. CNAMES\n",
    "5. MX Records (used for mails)\n",
    "6. PTR Records (opposite of A record i.e way of looking up from IP address to domain name)\n",
    "\n",
    "Routing Policies\n",
    "1. Simple routing\n",
    "2. Weighted routing\n",
    "3. Latency-based routing\n",
    "4. Failover routing\n",
    "5. Geolocation routing\n",
    "6. Geoproximity routing (traffic flow only) (complicate flow; not important)\n",
    "7. Multi-value answer routing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VPC\n",
    "\n",
    "- You can additionally create a hardware virtual private network connection between your corporate datacenter and your VPC and leverage AWS cloud as an extension of your datacenter\n",
    "\n",
    "- We have two lines of security in the VPC: NACL and SG\n",
    "    - NACL is stateless i.e they allow you to do allow rules and deny rules and we can block specific IP addresses\n",
    "    - SG is stateful\n",
    "    \n",
    "- Below is an example of VPC consisting of internet gateways, route table, NACL, SG, public subnet and private subnet.\n",
    "\n",
    "![vpc](./resources/vpc1.png)\n",
    "\n",
    "- The box in the top right of the diagram shows the ranges of private network allowed.\n",
    "\n",
    "__VPC Peering__\n",
    "- Allows you to connect one VPC with another via direct network route using private IP address\n",
    "- Instances behave as if they are on same network\n",
    "- You can peer VPCs with other AWS account and with other VPCs in the same account\n",
    "- You can peer between regions as well\n",
    "- Peering is in star configuration; NO TRANSITIVE PEERING\n",
    "\n",
    "> 1 subnet = 1 AZ; You can have multiple subnets in one AZ but not multiple subnets in one AZ\n",
    "\n",
    "> We can attach only 1 internet gateway to 1 VPC\n",
    "\n",
    "__DEFAULT__:\n",
    "- While creating VPC, we get __route table, NACL and SG__ by default\n",
    "\n",
    "![](./resources/default_vpc.png)\n",
    "\n",
    "- By default subnet created would have \"Auto Assign Public IP\" as No meaning it would be private. By this it means, any EC2 instance created in the subnet with \"Auto Assign Public IP\" set to No would not have public IP and thus can't be ssh'ed. So for any EC2 instance to be ssh'ed from public it needs to be set as Yes.\n",
    "\n",
    "> Interesting: My AZ name, say us-east-1a, could be completely different AZ from your us-east-1a as it is randomized by AWS so that everyone doesn't select us-east-1a as their default choice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exam Tips on EC2\n",
    "\n",
    "__Termination__\n",
    "- Termination protection is turned off by default. You must turn it on to protect from accidental termination.\n",
    "- On an EBS-backed instance (i.e with root volume), the default action is for the root EBS volume to be deleted on instance termination. Additional volume added to instance, won't be deleted by default. So, you have to checkmark the \"delete on termination\" to be effective.\n",
    "\n",
    "## Exam Tips on SG\n",
    "\n",
    "- Every time you change role in SG, it takes place immediately. For e.g let's say I disable http port, then that rules become effective immediately.\n",
    "- Multiple ec2 instances to SG and multiple SGs to an ec2 (many 2 many relationship b/w ec2 and sg).\n",
    "- SG are stateful i.e if you allow inbound rule allowing traffic in, that traffic is automatically allowed back out again.\n",
    "- You cannot block specific IP addres, use NACL for that.\n",
    "- All inbound rules blocked by default. You can specify allow rules but cannot deny rules.\n",
    "\n",
    "## Exam Tips on EBS and Snapshots\n",
    "\n",
    "- _How to move/copy ebs from one AZ to another?_ (__MIGRATION__):\n",
    "    1. Create its snapshot\n",
    "    2.1. Create image (AMI) that can be deployed to other AZs.\n",
    "    > Make sure you select Virtualization type as HVM as it gives us different instance type while creating the ec2 out of that image\n",
    "    \n",
    "    The image created goes under AMI and we can use that AMI to launch our instance in different subnet.\n",
    "    2.2. Create AMI and copy that to different AZ (while copying it asks for destination region)\n",
    "    \n",
    "> SUMMARY: create snapshot; create AMI and launch or create AMI and copy\n",
    "- Snapshots are stored on S3\n",
    "- Snapshots are incremental - this means only the blocks that have changed since your last snapshot are moved to S3\n",
    "  For e.g if you have f1 as file in the snapshot and next time you add f2 file in the volume and take snapshot then only the delta would be replicated to S3.\n",
    "- While creating snapshots especially for root volumes, best practice is to stop the instance and then create snapshot. However, you can create it on running instance as well.\n",
    "- AMIs from snapshot is possible\n",
    "- We can change volume size on the fly including volume type\n",
    "- Volume will ALWAYS be in the same AZ as the ec2 instance\n",
    "\n",
    "## Exam Tips on Instance Store Volumes\n",
    "- AMI root volume types can be either EBS or instance store\n",
    "- They are called Emphemeral Storage\n",
    "- Instance created wiht Instance Store Volume cannot be stopped unlike EBS. If underlying host fails, you will lose data. However, you can reboot your instance with this volume just like EBS.\n",
    "- By default, both ebs and isv root volumes are deleted on instance termination but with ebs we can change that.\n",
    "- we can add more instance store volumes only while creating the ec2 instance from ami with instance store as root but not after the ec2 instance has been created. However, we can add ebs volume to it after the instance is created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exam Tips for Network Adapters (ENI vs ENA vs EFA)\n",
    "\n",
    "Elastic Network Interface (ENI):\n",
    "- For basic networking. Perhaps you need a management network separate to your production network or a separate logging network and you need to do this at __low cost__. In this scenario use multiple ENIs for each network.\n",
    "\n",
    "Enhanced Network Adapter:\n",
    "- For when you need speed b/w __10Gbps to 100Gbps__. Anywhere you need reliable, high throughput.\n",
    "\n",
    "Elastic Fabric Adapter:\n",
    "- For when you need to accelerate __High Performance Computing (HPC)__ and __Machine Learning__ applications or if you need to do an __OS by-pass__. If you see a scenario question asking HPC, ML or OS by-pass, choose EFA\n",
    "- It supports only Linux and not Windows currently\n",
    "\n",
    "## Exam Tips for Encrypted Root Device Volumes and Snapshots\n",
    "- Snapshots of encrypted volumes are encrypted automatically\n",
    "- Volumes restored from encrypted snapshots are encrypted automatically\n",
    "- Snapshots can be shared only if they are unencrypted and these snapshots can be shared with other AWS account or made public\n",
    "\n",
    "To encrypt unencrypted root device volume\n",
    "1. Create its snapshot\n",
    "2. Create copy of that snapshot and select encryption option\n",
    "3. Create AMI from encrypted snapshot\n",
    "4. Use that AMI to launch new encrypted instances\n",
    "\n",
    "## Exam Tips for Spot Instances and Spot Fleet\n",
    "\n",
    "1. Spot instances save upto __90%__ of the cost of On-demand instances\n",
    "2. Use for applications __not__ requiring __data persistence__\n",
    "3. You can use __spot block__ to stop instances from terminating\n",
    "4. Spot fleet: __collection__ of spot instances, and optionally on-demand instances\n",
    "\n",
    "## Exam Tips on EC2 Hibernate\n",
    "\n",
    "- It preserves in-memory RAM on persistent EBS\n",
    "- Much faster to boot up as you __do not need to boot up OS__\n",
    "- Instance RAM must be __< 150GB__ and ebs bigger than RAM\n",
    "- Support c3, c4, c5, m3, m4, m5, r3, r4, r5 and some t2 instance types\n",
    "- Available for ubuntu, windows, amazon linux 2 ami\n",
    "- Instances can't be hibernated for more than __60 days__\n",
    "- Avaiable for __on demand__ and __reserved__ instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exam Tips on CloudWatch\n",
    "\n",
    "- Standard monitoring = 5mins\n",
    "- Detailed monitoring = 1min\n",
    "\n",
    "What can we do with CW?\n",
    "- Create awesome __dashboards__ (globally and regionally) to see what is happening\n",
    "- Create __alarms__ to notify when certain thresholds are hit\n",
    "- Create __events__ to allow us to respond to state changes in AWS resources\n",
    "- Create __logs__ to aggregate, monitor and store logs\n",
    "\n",
    "CloudWatch monitors performance while CloudTrail monitors API calls in the aws platform\n",
    "\n",
    "## Exam Tips on Roles\n",
    "\n",
    "- Roles are more secure than storing access key and secret access key on individual EC2 instances\n",
    "- roles are easier to manage (imagine you lose your access key and you have 1k instances running. you will have to go and update the access key for each instance)\n",
    "- roles can be assigned to ec2 instance after it is created using both console and cli\n",
    "- roleas are universal (all iam things are universal)\n",
    "\n",
    "## Exam Tips on bootstrap and metadata\n",
    "We can ssh to ec2 instance and get various info about user-data and meta-data for that instance.\n",
    "- Metadata can be used to get information about an instance (such as its ipv4, public-ipv4 etc)\n",
    "- curl http://169.254.169.254/latest/meta-data/\n",
    "- curl http://169.254.169.254/latest/user-data/\n",
    "- curl http://169.254.169.254/latest/meta-data/ > some.txt\n",
    "\n",
    "A small e.g is we can use user-data to write a shell script that runs this to get public ip, writes it to a file, then automatically copies it to s3 that could then trigger lambda function and then that could basically store that ip in a database.\n",
    "\n",
    "## Exam Tips on EFS\n",
    "\n",
    "- It supports Network File System version 4 (NFSv4) protocol\n",
    "- You can pay for the storage you use (no pre-provisioning required). In EBS we have to say we need 8GB or so.\n",
    "- Can scale up to petabytes\n",
    "- Can support thousands of concurrent NFS connections\n",
    "- Data is stored across multiple AZs within a region\n",
    "- Read after write consistency\n",
    "\n",
    "## Exam Tips on EC2 Placement Groups (falls under Network group in ec2 console)\n",
    "\n",
    "- Clusterd Placement Group\n",
    "    - Use case: for low latency and high network throughput\n",
    "- Spread Placement Group\n",
    "    - Use case: for individual critical EC2 instances\n",
    "    - Spread placement groups have a specific limitation that you can only have a maximum of 7 running instances per Availability Zone\n",
    "- Partitioned Placement Group\n",
    "    - Multiple EC2 instances\n",
    "    \n",
    "- Clustered PG can't spread over multiple AZs but Spread and Partitioned can but in same region\n",
    "- Cluster Placement Groups are primarily about keeping you compute resources within one network hop of each other on high speed rack switches. This is only helpful when you have compute loads with network loads that are either very high or very sensitive to latency.\n",
    "- PG name must be unique for an AWS account\n",
    "- No charge for creating PG\n",
    "- __Only certain type of instances can be launched within PG (Compute Optimized, GPU, Memory Optimized, Storage Optimized)__\n",
    "- AWS recommends homogeneous instance types withing Clustered PG\n",
    "- You can't merge PGs\n",
    "- You can move existing EC2 instance to and from placement group but the instance should be in stopped state. And they can be moved/removed only via AWS CLI or AWS SDK but not via console yet.\n",
    "\n",
    "> _My shortcut understanding: Only when we have certain optimized ec2 instances then only we can start thinking about whether we need PG or not_\n",
    "\n",
    "## Exam Tips on WAF\n",
    "\n",
    "- In the exam you'll be given different scenarios and you'll be asked how to block from malicious IP addresses\n",
    "    - Use WAF\n",
    "    - Use NACL (Network Access Control List)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exam Tips on RDS\n",
    "\n",
    "- RDS runs on virtual machines\n",
    "- You cannot login/ssh into these machines\n",
    "- Patching of the RDS operating system and DB is amazon's responsibility\n",
    "- RDS is __NOT SERVERLESS__\n",
    "- Aurora Serverless is Serverless\n",
    "\n",
    "## Exam Tips on RedShift\n",
    "\n",
    "- It's for business intelligence\n",
    "- Available only in 1 AZ\n",
    "\n",
    "__BACKUPS:__\n",
    "- Enabled by default with 1 day retention period\n",
    "- Just like RDS max retention period is 35 days\n",
    "- Redshift will attempt to make at least 3 copies of your data (the original and replica on compute node and a backup in S3)\n",
    "- Redshift can asynchronously replicate your snapshot to S3 in another region for DR (disaster recovery)\n",
    "\n",
    "## Exam Tips on Aurora\n",
    "\n",
    "- 2 copies of your data are contained in each AZ with minimum of 3 AZ. 6 copies of your data\n",
    "- 3 types of replicas available. Aurora replicas, MySQL replicas and PostgreSQL replicas. Automated failover is only available with Aurora Replicas\n",
    "- Aurora has automated backups turned on by default. You can take snapshots and share it with other AWS accounts (while taking snapshots it's not gonna effect your production database)\n",
    "- Use Aurora serverless if you want a simple, cost effective option for infrequent, intermittent or unpredicatable workloads\n",
    "\n",
    "## Exam Tips on Elastic Cache\n",
    "\n",
    "- Use Case: let's say you query top 10 purchases in amazon, so ec2 instances query the elastic cache instead of the production database environment. Elastic Cache stores that info in there (as that query changes very infrequently). This improves the performance as it is fast to query from cache. You cache your most important queries in Elastic Cache.\n",
    "- So, it is used to increase database and web application performance\n",
    "- A common question is what would you do if your database is overloaded. One is you use read replicas and the other elastic cache\n",
    "- Memcached is for simple scenarios while redis is for advanced data types and redis is multi AZ and has backups and restore features.\n",
    "\n",
    "## Exam Tips on DMS\n",
    "\n",
    "- Allows migration of databases from one source to AWS and vice-versa\n",
    "- The source can be either on-premise, or within AWS or another cloud provider such as Azure\n",
    "- You can do __homogenous__ migrations (same DB engines) or __heterogenous__ migrations\n",
    "- If you do heteregenous migration, you will need __AWS Schema Conversion Tool (SCT)__\n",
    "\n",
    "## Exam Tips on EMR (Elastic Map Reduce)\n",
    "\n",
    "- EMR is used for big data processing\n",
    "- Consists of a master node, core node and optionally task node\n",
    "- By default, log data is stored on master node\n",
    "- You can configure replication to S3 __five minutes interval for all log data from the master node__; however this can only be configured when creating the cluster for first time (this step prevents your data loss from master node failure or termination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exam Tips on IAM Policies\n",
    "\n",
    "- Not explicitly allowed = Implicitly denied\n",
    "- Explicit deny > everything else\n",
    "- Only attached policies (to user, role) have effect\n",
    "- AWS joins all applicable policies (if we have attached 2 or more policies to a role/user)\n",
    "- We have AWS managed and customer managed policies\n",
    "\n",
    "__Permission boundaries__\n",
    "- they do not grant access but limit the access that the user/role already has\n",
    "- Used to delegate administration access to other users\n",
    "- Prevent privilege escalation and unnecessary broad permissions\n",
    "- Control maximum permissions an IAM policy can grant\n",
    "- Use Cases:\n",
    "    - Developers creating roles for lambda functions\n",
    "    - Application owners creating roles for ec2 instances\n",
    "    - Admins creating ad hoc users\n",
    "    \n",
    "## Exam Tips on Route53\n",
    "- It can take upto 3 days to register domain name depending on the circumstances\n",
    "- _Simple Routing Policy_\n",
    "    - if we choose simple routing policy, we can have only one record with multiple IP addresses. If you specify mulitple values in a record, Route 53 returns all values to the user in a random order based on TTL set.\n",
    "- _Weighted Routing Policy_\n",
    "    - allows you to split your traffic based on weights assigned\n",
    "    \n",
    "    __Heath Checks__\n",
    "    - You can set health checks on individual record sets\n",
    "    - If a record set fails a health check it will be removed from Route 53 until it passes the health check\n",
    "    - You can set SNS notifications to alert you if health check is failed\n",
    "- _Latency Routing_\n",
    "    - sends traffic to your end-user based on lowest network latency\n",
    "- _Failover Routing_\n",
    "    - we create active-passive setup\n",
    "- _Geolocation Routing_\n",
    "    - routes traffic based on end-user location\n",
    "    - can choose region for end-users based on continent as well as country\n",
    "- _Geoproximity Routing (Traffic Flow only)_\n",
    "    - must use Route 53 traffic flow\n",
    "- _Multivalue Answer Routing_\n",
    "    - similar to simple routing policy except that we have health check option and also instead of having multiple values for the record set, we can have multiple record set.\n",
    "    \n",
    "---\n",
    "\n",
    "- ELBs do not have predefined IPv4 addresses; you resolve to them using DNS name\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exam Tips on VPC\n",
    "\n",
    "- When we create a VPC a default Route Table, NACL and SG are created\n",
    "- It won't create a subnet nor an internet gateway\n",
    "- us-east-1a in one AWS account could be a completely different AZ to us-east-1a in another AWS account. AZ's are randomized\n",
    "- Amazon resevers 5 IP addresses within your subnets\n",
    "- 1 internet gateway per VPC\n",
    "- Security Groups can't span VPCs; meaning SG created in one VPC can't be seen in another VPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTIONS/DOUBTS/NOTES\n",
    "\n",
    "- If an Amazon EBS volume is an additional partition (not the root volume), can I detach it without stopping the instance? __Yes__, although it may take some time.\n",
    "- Can I delete a snapshot of an EBS Volume that is used as the root device of a registered AMI? __No__.\n",
    "- Which AWS CLI command should I use to create a snapshot of an EBS volume? __aws ec2 create-snapshot__\n",
    "- Can you attach an EBS volume to more than one EC2 instance at the same time? __As of Feb 2020 you can attach certain types of EBS volumes to multiple EC2 instances. https://aws.amazon.com/blogs/aws/new-multi-attach-for-provisioned-iops-io1-amazon-ebs-volumes/__\n",
    "-If I wanted to run a database on an EC2 instance, which of the following storage options would Amazon recommend? __EBS__\n",
    "- MySQL installations default to port number 3306\n",
    "- If you want your application to check RDS for an error, have it look for an ______ node in the response from the Amazon RDS API: __error__\n",
    "- What data transfer charge is incurred when replicating data from your primary RDS instance to your secondary RDS instance? __no charge__\n",
    "- If you are using Amazon RDS Provisioned IOPS storage with a Microsoft SQL Server database engine, what is the maximum size RDS volume you can have by default? __16TB__\n",
    "- You are hosting a MySQL database on the root volume of an EC2 instance. The database is using a large number of IOPS, and you need to increase the number of IOPS available to it. What should you do? __Add 4 additional ESB SSD volumes and create RAID 10 using these volumes__\n",
    "- When you add a rule to an RDS DB security group, you must specify a port number or protocol. __False. Technically a destination port number is needed, however with a DB security group the RDS instance port number is automatically applied to the RDS DB Security Group.__\n",
    "- There is a limit to the number of domain names that you can manage using Route 53. Default limit is 50 but you can increase it by contacting AWS support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
